# Cambridge Analytica: Privacy & User Mechanics

### Introduction
On the 17th of March, 2018, whistleblower Christopher Wylie of data firm Cambridge Analytica revealed to the world the harvesting of critical personal data of more than 87 million Facebook users to Cambridge Analytica. Their agenda; to gather personality and psychological profiles of individuals for commercial and political gain, breaching data use recommendations and risking individuals' right to consent and privacy (Isaak & Hanna, 2018). The advancement of technology and its infiltration into our everyday lives has caused the creation of an astronomical amount of sensitive and private algorithmic big data and a growing concern about the privacy impacts on society in the individual, commercial and governmental sectors. 
Unfortunately, policy updates like co-regulation and shared governance seeking to regulate user consent and data protection tend to lag behind the astronomical growth of online platforms. More research on platformisation, its influence and its impacts is needed along with global recognition of the need for cooperation in this advancing area of security and data threats (Flew & Martin, 2022). 

### Cause and Consequences
2013 Cambridge University researchers drew in 350,000 participants to take a personality test ‘OCEAN’ on Facebook. This research established a correlation between the individual's personality type and their activity on Facebook. Cambridge Analytica then commenced a second project to create a methodology for psychographic profiling based on social media (Isaak & Hanna, 2018). They procured 5000 + data points on 230 million US citizens and were able to then micro-target individuals by adding ‘OCEAN’ analysis to that data to influence their behaviour on social media platforms and browsers. This was then used in ‘Project Almo’ to disrupt the US elections to tip results along with allegations on the Brexit Referendum. Targeted manipulation based on the harvest of millions of Facebook profiles allowed the Cambridge Analytical algorithms to identify and target possible swing voters and potentially manipulate their voting behaviour Cadwalladr & Graham, 2018). Christopher Wylie himself quotes “We exploited Facebook to harvest millions of profiles and built models to exploit that and target their inner demons.” this exposes Cambridge Analytica and Facebook to the enormous breach of public trust and disregard for individual privacy. 

Investigations by the British Information Commissioners Office examined the role that Cambridge Analytica played in the EU referendum via the use of data in the micro-targeting of voters (Cadwalladr & Graham, 2018). Demonstrating how to target advertising and misinformation ‘fake news’ could be seen as a threat to democracy in its ability to cause social fragmentation (Flew & Martin, 2022).  
The Australian Digital Platforms Inquiry Report in 2019 post Cambridge Analytica scandal concludes that digital platforms need to take responsibility for their users' rights to access quality content and news (Flew & Martin, 2022). 
The monetisation of data and in turn, mistrust of digital services leaves society unprotected the the wake of such scandals. Profitable fake news and its predatory viral nature damage users' trust in digital platforms and their outlook on the news. The opportunity that these platforms provide for big data firms like Cambridge Analytica for the study of digital footprints and behaviours is a gold mine for micro-targeting advertisements (Hinds et al., 2020), however, its breach of privacy due to lax Facebook self-regulated privacy policies has caused a massive violation of individual privacy in this case leading to the destruction of trust and reputation among all involved highlighting the power and danger of unregulated consent and privacy guidelines. 

### Legal and Commercial Factors 
Federal Trade Commission penalty fines of $5 billion were imposed on Facebook by July 2019 along with a corporation value drop of $45 billion USD, in the scheme of such a global tech giant like Meta this $5 billion settlement is barely a slap on the wrist. The limits of the FTC authority however are limited to ‘unfair or deceptive acts or practices in or affecting commerce’. Facebook settled the investigation against breach of disclosure of private user data to third parties and under FTC order to review and update their organisational privacy policies. Commissioner Rohit Chopra expressed the need for transparency in the breakdown of the fines, analysis of the penalties and further insight into the need to take such cases to a public court of law for the need to effect more influential change in tech giant breaches (Hu, 2020). Further resources and in-house expertise should be considered for commissions like the FTC in order to effectively investigate and provide insightful change within data protection laws. 

Facebook denied that the harvested data was considered a data breach, maintaining the fact that the third party collected data under regulations but did not abide by rules that denied the passing on of information to third parties (Cadwalladr & Graham, 2018). 
Following the Cambridge Analytica scandal and Mark Zuckerberg testifying on Facebook's privacy policies bills were pushed forward covering Consent and data privacy issues. The Consent Act requires privacy protections for individuals and opt-in consent from social media platforms before their data is used or sold to any third-party platforms. The fact that Cambridge Analytica were able to not only collect the data of the Facebook users via the psychological profiling survey but also the user's friends data also reveals the need for major reform (Schneble et al., 2018). The Social Media  Privacy Protection and Consumer Rights Act (2018) is across similar factors but adds restrictions on modifications to privacy policies and notification of data control procedures (Isaak & Hanna, 2018). 

Push for Californian bills sparked an international push for the European Union General Data Protection Regulation, now holding the most specific and practical terms of data protection and privacy (Lapowsky, 2018). The GDPR in effect from 25th May 2018 now has comprehensive guidelines on user consent. Article 7 states that “the request for consent shall be presented in a manner which is clearly distinguishable from other matters, in an intelligible and easily accessible form, using clear and plain language… any part of such a declaration, which constitutes an infringement of the Regulation shall not be binding.” (Schneble et al., 2018)

The failure to take social responsibility for the data produced through major tech companies like Facebook could stem from the policymaker's failure to provide significant and clear guidelines and practical support for companies handling the data of individuals and organisations. Such regulations that cover traditional advertising like television should provide the same regulations for digital platform advertising (Cadwalladr & Graham, 2018). The same could be said for the failure to adhere to ethical research regulations. In traditional psychology and medicine research project a user would give explicit consent for the use of any data collection and third-party use, this traditional institutional method of maintaining essential user consent has not transferred to data science research projects due to its sometimes enormous amount of data and participants (Schneble et al., 2018). This unregulated and unethical gathering of personal data without consent by Cambridge Analytica and it’s allowance by tech giant Facebook/Meta highlights a severe lack of ethical consent standards in the data science industry. 

Facebook is driven by a behavioural advertising model, essentially subliminally selling the user to advertisers. This model although allows the online platform to be free, targets users based on psychographic profiling as revealed in the Cambridge Analytica scandal (Flew & Martin, 2022). Neglect of third-party monitoring or auditing and the allowance of the collection of external networks of ‘friends’ data harvesting. 
The failure of Cambridge Analytica and Facebook to self-regulate lies in the problem of regulation being in the interest of the internal company rather than an external auditing process. Boundaries and guidelines may remain unclear or be overlooked in the company's interest. Lack of ethical framework, prioritising advertising or political gains and relationships over individual right to privacy. Facebook being driven by behavioural advertising allows its users to become a psychological profiling target for third parties, hanging its users out to dry via its neglect of internal data regulations. 

### Future Implications and Recommendations 
Regulation, support and legal ramifications for abuse of responsible data protection will enforce accountability and responsibility on social platforms and those third parties who have access to their data along with the development of specific guidelines for internet-mediated big data research (Schneble et al., 2018). Transparency within the deployment of algorithm-based data sets, especially those that directly impact the violation of user consent and data collection should be embedded within regulatory policies and be enforced by Policymakers such as the FTC (Hu, 2020).  

Transparency surrounding business activities is essential to bring trust in big tech back to the public eye. For CEO of Cambridge Analytica to give false information on the collection of data spurred a rabbit hole of deception underlying the harvesting and use of Facebook data. Facebook vice president informed appropriate parties that they are committed to enforcing their data  policies to protect users information (Cadwalladr & Graham, 2018). 
Solutions to jurisdiction boundaries due to limited international policy are required for global digital platforms in the form of harmonisation of policy across states and countries (Flew & Martin, 2022).  A combination of self-regulation in which an organisation must develop their own auditing practices and regulation standards based on responsible data privacy best practices along with approval or regular auditing by governments (Flew & Martin, 2022). 
Fines, although a reputational deterrent often do not have the required result on massive tech companies like Facebook, there needs to be some other measures in place for such breaches. 

Users must have full transparency in data collection and use along with the ability to opt-out and block control in all third-party sites. Violations of data protection policies must be met with legal action or audits. Support and education should be continuously given to individual users of digital platforms in this context, to reduce any sort of ‘privacy fatigue’ where overwhelming information about privacy and breaches cause users to become blase about their privacy and consent in online platforms or evaluate revealing private data for certain trade-offs online (Hinds et al., 2020). The elimination or regulation of networked privacy data gathering also plays a part in the future of social media guidelines, where a user may reveal elements of their extended network such as location data without explicit consent from the individual. 

The push for the adoption of the GDPR or similar globally with a view of security and privacy of data and individual privacy rights by default is a step in the right direction, however, continuous reviews, research and support surrounding data and user consent laws and regulations are still necessary for the advancement and protection of both the user and organisations. Comprehensive privacy law reform is necessary along with the global adoption of ethical data protection frameworks and security by design concepts (Hu, 2020). 

### Conclusion 
Conclusively, the Cambridge Analytica scandal of 2018 was a violation of individual data privacy and consent rights. The failure of Facebook's third-party data regulations along with the harvesting of Facebook user's networks without consent or consideration of ethical data privacy resulted in a call for international guideline collaboration and full user consent transparency via joint regulation and governance in digital platforms and big data research. 


### References
Cadwalladr, C., & Graham, E. (2018, March 17). Revealed: 50 million Facebook profiles harvested for Cambridge Analytica in major data breach. The Guardian. Retrieved November 25, 2023, from https://www.theguardian.com/news/2018/mar/17/cambridge-analytica-facebook-influence-us-election

Flew, T., & Martin, F. (Eds.). (2022). Digital Platform Regulation: Global Perspectives on Internet Governance. Springer International Publishing. 10.1007/978-3-030-95220-4

Hinds, J., Williams, E., & Joinson, A. (2020, November). It wouldn't happen to me: Privacy concerns and perspectives following the Cambridge Analytica scandal. International Journal of Human Computer Studies, 143. Science Direct. https://doi.org/10.1016/j.ijhcs.2020.102498

Hu, M. (2020, August 24). Cambridge Analytica's black box. Big Data & Society, 7. https://doi.org/10.1177/2053951720938091

Isaak, J., & Hanna, M. (2018, August). User Data Privacy: Facebook, Cambridge Analytica, and Privacy Protection. Computer, 51(8), 56-59. IEEE Xplore. 10.1109/MC.2018.3191268

Lapowsky, I. (2018, June 22). Bill Could Give Californians Unprecedented Control Over Data. WIRED. Retrieved November 25, 2023, from https://www.wired.com/story/new-privacy-bill-could-give-californians-unprecedented-control-over-data/

Schneble, C., Elger, B., & Shaw, D. (2018, August 1). The Cambridge Analytica affair and Internet mediated Research. EMBO Reports, 19(8). https://doi.org/10.15252/embr.201846579

Wagner, P. (2021, February 9). Data Privacy - The Ethical, Sociological and Philosophical Effects of Cambridge Analytica. Compliance & Risk Managment. https://doi.org/10.2139/ssrn.3782821
